{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608f9f95",
   "metadata": {},
   "source": [
    "# Nestlé HR Policy Chatbot\n",
    "\n",
    "This notebook demonstrates how to build a conversational chatbot that answers questions based on Nestlé’s HR policy document.  \n",
    "\n",
    "The workflow consists of:\n",
    "- Loading the PDF and splitting it into manageable chunks.  \n",
    "- Embedding the text chunks into a vector space.  \n",
    "- Storing and querying embeddings using a Chroma vector store.  \n",
    "- Leveraging OpenAI’s GPT model for retrieval‑augmented question answering.  \n",
    "- Creating a user‑friendly interface with Gradio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages (run once)\n",
    "!pip install --quiet openai langchain chromadb pypdf gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3687611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY'\n",
    "\n",
    "# Path to the Nestlé HR policy PDF (update the path as needed)\n",
    "pdf_path = 'the_nestle_hr_policy_pdf_2012.pdf'\n",
    "\n",
    "# Load and split the PDF into pages\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Further split the pages into smaller chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "print(f\"Loaded {len(docs)} document chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Create embeddings and build a Chroma vector database\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore_path = './chroma_db'\n",
    "\n",
    "# If a persistent database exists, load it; otherwise, create a new one\n",
    "if os.path.exists(vectorstore_path):\n",
    "    db = Chroma(persist_directory=vectorstore_path, embedding_function=embeddings)\n",
    "else:\n",
    "    db = Chroma.from_documents(docs, embeddings, persist_directory=vectorstore_path)\n",
    "    db.persist()\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={'k': 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab969303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Build the retrieval‑augmented question answering chain\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Define a function to answer questions\n",
    "def answer_question(query: str) -> str:\n",
    "    '''Return an answer to the query based on the HR policy.'''\n",
    "    result = qa_chain(query)\n",
    "    return result['result']\n",
    "\n",
    "# Test the function\n",
    "# print(answer_question('What is the policy on parental leave?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acaa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Gradio chat interface\n",
    "def chatbot_interface(question: str) -> str:\n",
    "    return answer_question(question)\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=chatbot_interface,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about Nestlé’s HR policy...\"),\n",
    "    outputs='text',\n",
    "    title='Nestlé HR Policy Chatbot',\n",
    "    description='Ask me about the Nestlé HR policy and I will answer your questions based on the official document.'\n",
    ")\n",
    "\n",
    "# To launch the Gradio app, uncomment the line below and run the cell\n",
    "# iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34ac4f",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Replace `YOUR_OPENAI_API_KEY` with your actual OpenAI API key before running the notebook.  \n",
    "- Make sure the PDF file (`the_nestle_hr_policy_pdf_2012.pdf`) is in the same directory as this notebook or update the `pdf_path` variable accordingly.  \n",
    "- The vector database is saved in a folder called `chroma_db` for persistence between sessions.  \n",
    "- To deploy the Gradio interface for others to use, call `iface.launch()` and follow the instructions.  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
