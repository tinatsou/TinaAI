{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad7165a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "import numpy as np\n",
    "import json\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Any, List, Tuple, Optional, Callable\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "# Load .env from either current dir or parent (for OPENAI_API_KEY, etc.)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(dotenv_path=\".env\"); load_dotenv(dotenv_path=\"../.env\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "random.seed(7); np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564bec08",
   "metadata": {},
   "source": [
    "#### How to pass a single tool to LLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f80a6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str, units: str) -> int:\n",
    "    \"\"\"\n",
    "    Fetches the current weather for a given location and returns the temperature in the specified units.\n",
    "\n",
    "    Parameters:\n",
    "    location (str): The location to fetch the weather for.\n",
    "    units (str): The units for temperature (celsius or fahrenheit).\n",
    "\n",
    "    Returns:\n",
    "    int: The current temperature in the specified units.\n",
    "    \"\"\"\n",
    "    if units not in ['celsius', 'fahrenheit']:\n",
    "        raise ValueError(\"Units must be either 'celsius' or 'fahrenheit'.\")\n",
    "    else:\n",
    "        if units == 'celsius':\n",
    "            return 22\n",
    "        else:\n",
    "            return 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Retrieves current weather for the given location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
    "                    },\n",
    "                    \"units\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"Units the temperature will be returned in.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\", \"units\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "client.chat.completions.create(\n",
      "    *,\n",
      "    messages: \u001b[33m'Iterable[ChatCompletionMessageParam]'\u001b[39m,\n",
      "    model: \u001b[33m'Union[str, ChatModel]'\u001b[39m,\n",
      "    audio: \u001b[33m'Optional[ChatCompletionAudioParam] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    frequency_penalty: \u001b[33m'Optional[float] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    function_call: \u001b[33m'completion_create_params.FunctionCall | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    functions: \u001b[33m'Iterable[completion_create_params.Function] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    logit_bias: \u001b[33m'Optional[Dict[str, int]] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    logprobs: \u001b[33m'Optional[bool] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    max_completion_tokens: \u001b[33m'Optional[int] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    max_tokens: \u001b[33m'Optional[int] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    metadata: \u001b[33m'Optional[Metadata] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    modalities: \u001b[33m\"Optional[List[Literal['text', 'audio']]] | NotGiven\"\u001b[39m = NOT_GIVEN,\n",
      "    n: \u001b[33m'Optional[int] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    parallel_tool_calls: \u001b[33m'bool | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    prediction: \u001b[33m'Optional[ChatCompletionPredictionContentParam] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    presence_penalty: \u001b[33m'Optional[float] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    prompt_cache_key: \u001b[33m'str | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    reasoning_effort: \u001b[33m'Optional[ReasoningEffort] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    response_format: \u001b[33m'completion_create_params.ResponseFormat | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    safety_identifier: \u001b[33m'str | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    seed: \u001b[33m'Optional[int] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    service_tier: \u001b[33m\"Optional[Literal['auto', 'default', 'flex', 'scale', 'priority']] | NotGiven\"\u001b[39m = NOT_GIVEN,\n",
      "    stop: \u001b[33m'Union[Optional[str], List[str], None] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    store: \u001b[33m'Optional[bool] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    stream: \u001b[33m'Optional[Literal[False]] | Literal[True] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    stream_options: \u001b[33m'Optional[ChatCompletionStreamOptionsParam] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    temperature: \u001b[33m'Optional[float] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    tool_choice: \u001b[33m'ChatCompletionToolChoiceOptionParam | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    tools: \u001b[33m'Iterable[ChatCompletionToolUnionParam] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    top_logprobs: \u001b[33m'Optional[int] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    top_p: \u001b[33m'Optional[float] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    user: \u001b[33m'str | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    verbosity: \u001b[33m\"Optional[Literal['low', 'medium', 'high']] | NotGiven\"\u001b[39m = NOT_GIVEN,\n",
      "    web_search_options: \u001b[33m'completion_create_params.WebSearchOptions | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      "    extra_headers: \u001b[33m'Headers | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    extra_query: \u001b[33m'Query | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    extra_body: \u001b[33m'Body | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    timeout: \u001b[33m'float | httpx.Timeout | None | NotGiven'\u001b[39m = NOT_GIVEN,\n",
      ") -> \u001b[33m'ChatCompletion | Stream[ChatCompletionChunk]'\u001b[39m\n",
      "\u001b[31mDocstring:\u001b[39m <no docstring>\n",
      "\u001b[31mSource:\u001b[39m   \n",
      "    @required_args([\u001b[33m\"messages\"\u001b[39m, \u001b[33m\"model\"\u001b[39m], [\u001b[33m\"messages\"\u001b[39m, \u001b[33m\"model\"\u001b[39m, \u001b[33m\"stream\"\u001b[39m])\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m create(\n",
      "        self,\n",
      "        *,\n",
      "        messages: Iterable[ChatCompletionMessageParam],\n",
      "        model: Union[str, ChatModel],\n",
      "        audio: Optional[ChatCompletionAudioParam] | NotGiven = NOT_GIVEN,\n",
      "        frequency_penalty: Optional[float] | NotGiven = NOT_GIVEN,\n",
      "        function_call: completion_create_params.FunctionCall | NotGiven = NOT_GIVEN,\n",
      "        functions: Iterable[completion_create_params.Function] | NotGiven = NOT_GIVEN,\n",
      "        logit_bias: Optional[Dict[str, int]] | NotGiven = NOT_GIVEN,\n",
      "        logprobs: Optional[bool] | NotGiven = NOT_GIVEN,\n",
      "        max_completion_tokens: Optional[int] | NotGiven = NOT_GIVEN,\n",
      "        max_tokens: Optional[int] | NotGiven = NOT_GIVEN,\n",
      "        metadata: Optional[Metadata] | NotGiven = NOT_GIVEN,\n",
      "        modalities: Optional[List[Literal[\u001b[33m\"text\"\u001b[39m, \u001b[33m\"audio\"\u001b[39m]]] | NotGiven = NOT_GIVEN,\n",
      "        n: Optional[int] | NotGiven = NOT_GIVEN,\n",
      "        parallel_tool_calls: bool | NotGiven = NOT_GIVEN,\n",
      "        prediction: Optional[ChatCompletionPredictionContentParam] | NotGiven = NOT_GIVEN,\n",
      "        presence_penalty: Optional[float] | NotGiven = NOT_GIVEN,\n",
      "        prompt_cache_key: str | NotGiven = NOT_GIVEN,\n",
      "        reasoning_effort: Optional[ReasoningEffort] | NotGiven = NOT_GIVEN,\n",
      "        response_format: completion_create_params.ResponseFormat | NotGiven = NOT_GIVEN,\n",
      "        safety_identifier: str | NotGiven = NOT_GIVEN,\n",
      "        seed: Optional[int] | NotGiven = NOT_GIVEN,\n",
      "        service_tier: Optional[Literal[\u001b[33m\"auto\"\u001b[39m, \u001b[33m\"default\"\u001b[39m, \u001b[33m\"flex\"\u001b[39m, \u001b[33m\"scale\"\u001b[39m, \u001b[33m\"priority\"\u001b[39m]] | NotGiven = NOT_GIVEN,\n",
      "        stop: Union[Optional[str], List[str], \u001b[38;5;28;01mNone\u001b[39;00m] | NotGiven = NOT_GIVEN,\n",
      "        store: Optional[bool] | NotGiven = NOT_GIVEN,\n",
      "        stream: Optional[Literal[\u001b[38;5;28;01mFalse\u001b[39;00m]] | Literal[\u001b[38;5;28;01mTrue\u001b[39;00m] | NotGiven = NOT_GIVEN,\n",
      "        stream_options: Optional[ChatCompletionStreamOptionsParam] | NotGiven = NOT_GIVEN,\n",
      "        temperature: Optional[float] | NotGiven = NOT_GIVEN,\n",
      "        tool_choice: ChatCompletionToolChoiceOptionParam | NotGiven = NOT_GIVEN,\n",
      "        tools: Iterable[ChatCompletionToolUnionParam] | NotGiven = NOT_GIVEN,\n",
      "        top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n",
      "        top_p: Optional[float] | NotGiven = NOT_GIVEN,\n",
      "        user: str | NotGiven = NOT_GIVEN,\n",
      "        verbosity: Optional[Literal[\u001b[33m\"low\"\u001b[39m, \u001b[33m\"medium\"\u001b[39m, \u001b[33m\"high\"\u001b[39m]] | NotGiven = NOT_GIVEN,\n",
      "        web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n",
      "        \u001b[38;5;66;03m# Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# The extra values given here take precedence over values defined on the client or passed to this method.\u001b[39;00m\n",
      "        extra_headers: Headers | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        extra_query: Query | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        extra_body: Body | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        timeout: float | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n",
      "    ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n",
      "        validate_response_format(response_format)\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._post(\n",
      "            \u001b[33m\"/chat/completions\"\u001b[39m,\n",
      "            body=maybe_transform(\n",
      "                {\n",
      "                    \u001b[33m\"messages\"\u001b[39m: messages,\n",
      "                    \u001b[33m\"model\"\u001b[39m: model,\n",
      "                    \u001b[33m\"audio\"\u001b[39m: audio,\n",
      "                    \u001b[33m\"frequency_penalty\"\u001b[39m: frequency_penalty,\n",
      "                    \u001b[33m\"function_call\"\u001b[39m: function_call,\n",
      "                    \u001b[33m\"functions\"\u001b[39m: functions,\n",
      "                    \u001b[33m\"logit_bias\"\u001b[39m: logit_bias,\n",
      "                    \u001b[33m\"logprobs\"\u001b[39m: logprobs,\n",
      "                    \u001b[33m\"max_completion_tokens\"\u001b[39m: max_completion_tokens,\n",
      "                    \u001b[33m\"max_tokens\"\u001b[39m: max_tokens,\n",
      "                    \u001b[33m\"metadata\"\u001b[39m: metadata,\n",
      "                    \u001b[33m\"modalities\"\u001b[39m: modalities,\n",
      "                    \u001b[33m\"n\"\u001b[39m: n,\n",
      "                    \u001b[33m\"parallel_tool_calls\"\u001b[39m: parallel_tool_calls,\n",
      "                    \u001b[33m\"prediction\"\u001b[39m: prediction,\n",
      "                    \u001b[33m\"presence_penalty\"\u001b[39m: presence_penalty,\n",
      "                    \u001b[33m\"prompt_cache_key\"\u001b[39m: prompt_cache_key,\n",
      "                    \u001b[33m\"reasoning_effort\"\u001b[39m: reasoning_effort,\n",
      "                    \u001b[33m\"response_format\"\u001b[39m: response_format,\n",
      "                    \u001b[33m\"safety_identifier\"\u001b[39m: safety_identifier,\n",
      "                    \u001b[33m\"seed\"\u001b[39m: seed,\n",
      "                    \u001b[33m\"service_tier\"\u001b[39m: service_tier,\n",
      "                    \u001b[33m\"stop\"\u001b[39m: stop,\n",
      "                    \u001b[33m\"store\"\u001b[39m: store,\n",
      "                    \u001b[33m\"stream\"\u001b[39m: stream,\n",
      "                    \u001b[33m\"stream_options\"\u001b[39m: stream_options,\n",
      "                    \u001b[33m\"temperature\"\u001b[39m: temperature,\n",
      "                    \u001b[33m\"tool_choice\"\u001b[39m: tool_choice,\n",
      "                    \u001b[33m\"tools\"\u001b[39m: tools,\n",
      "                    \u001b[33m\"top_logprobs\"\u001b[39m: top_logprobs,\n",
      "                    \u001b[33m\"top_p\"\u001b[39m: top_p,\n",
      "                    \u001b[33m\"user\"\u001b[39m: user,\n",
      "                    \u001b[33m\"verbosity\"\u001b[39m: verbosity,\n",
      "                    \u001b[33m\"web_search_options\"\u001b[39m: web_search_options,\n",
      "                },\n",
      "                completion_create_params.CompletionCreateParamsStreaming\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m stream\n",
      "                \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n",
      "            ),\n",
      "            options=make_request_options(\n",
      "                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n",
      "            ),\n",
      "            cast_to=ChatCompletion,\n",
      "            stream=stream \u001b[38;5;28;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "            stream_cls=Stream[ChatCompletionChunk],\n",
      "        )\n",
      "\u001b[31mFile:\u001b[39m      ~/opt/anaconda3/envs/ai_project/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "client.chat.completions.create??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e3ecf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_mUhDsbx0AdkEGnAPUDwUinCG', function=Function(arguments='{\"location\":\"Paris, France\",\"units\":\"celsius\"}', name='get_weather'), type='function')])\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "system_prompt = \"Fetch the weather temperature for the user.\"\n",
    "user_prompt = \"What is the weather in Paris in celsius?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "msg = resp.choices[0].message\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "486d7055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Fetch the weather temperature for the user.'}, {'role': 'user', 'content': 'What is the weather in Paris in celsius?'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_mUhDsbx0AdkEGnAPUDwUinCG', 'type': 'function', 'function': {'name': 'get_weather', 'arguments': '{\"location\": \"Paris, France\", \"units\": \"celsius\"}'}}]}, {'role': 'tool', 'tool_call_id': 'call_mUhDsbx0AdkEGnAPUDwUinCG', 'content': '{\"temperature\": 22, \"units\": \"celsius\"}'}]\n",
      "The current temperature in Paris is 22°C.\n"
     ]
    }
   ],
   "source": [
    "if msg.tool_calls:\n",
    "    tc = msg.tool_calls[0]\n",
    "    args = json.loads(tc.function.arguments or \"{}\")\n",
    "    result = get_weather(**args)\n",
    "\n",
    "    # append tool call & result\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"tool_calls\": [\n",
    "            {\"id\": tc.id, \"type\": \"function\",\n",
    "             \"function\": {\"name\": tc.function.name, \"arguments\": json.dumps(args)}}\n",
    "        ]\n",
    "    })\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tc.id,\n",
    "        \"content\": json.dumps({\"temperature\": result, \"units\": args[\"units\"]})\n",
    "    })\n",
    "\n",
    "    print(messages)\n",
    "\n",
    "    # 5) Ask the model for the final natural-language answer\n",
    "    final = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    print(final.choices[0].message.content)\n",
    "else:\n",
    "    # Model chose to answer without tools\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d738d1",
   "metadata": {},
   "source": [
    "#### How to pass multiple tools to LLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c67759e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review(place: str, limit: int = 3) -> dict:\n",
    "    # Dummy sample\n",
    "    return {\"place\": place, \"reviews\": [f\"Review {i+1} for {place}\" for i in range(limit)]}\n",
    "\n",
    "TOOLS_REGISTRY: Dict[str, Callable[[Dict[str, Any]], Any]] = {\n",
    "    \"get_weather\": lambda args: get_weather(args[\"location\"], args[\"units\"]),\n",
    "    \"get_review\": lambda args: get_review(args[\"place\"], args.get(\"limit\", 3)),\n",
    "}\n",
    "\n",
    "TOOLS_SCHEMA = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current temperature for a location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\"},\n",
    "                    \"units\": {\"type\": \"string\", \"enum\": [\"celsius\",\"fahrenheit\"]}\n",
    "                },\n",
    "                \"required\": [\"location\",\"units\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_review\",\n",
    "            \"description\": \"Fetch reviews for a place.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"place\": {\"type\": \"string\"},\n",
    "                    \"limit\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10}\n",
    "                },\n",
    "                \"required\": [\"place\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a2d996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(\n",
    "    messages: List[Dict[str, Any]],\n",
    "    tools_schema: List[Dict[str, Any]],\n",
    "    tool_registry: Dict[str, Callable[[Dict[str, Any]], Any]],\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    max_steps: int = 8,\n",
    "    truncate_tool_result_chars: int = 15000,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Loop until the model stops calling tools or max_steps reached.\n",
    "    Returns {\"messages\": [...], \"final_text\": str | None}\n",
    "    \"\"\"\n",
    "    for step in range(max_steps):\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools_schema,\n",
    "            tool_choice=\"auto\",\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        msg = resp.choices[0].message\n",
    "\n",
    "        # If model returns final text (no tool calls), we’re done\n",
    "        tool_calls = getattr(msg, \"tool_calls\", None)\n",
    "        print(tool_calls)\n",
    "        if not tool_calls:\n",
    "            return {\"messages\": messages + [{\"role\": \"assistant\", \"content\": msg.content}], \"final_text\": msg.content}\n",
    "\n",
    "        # Append the assistant tool_calls message itself\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": tc.id,\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\"name\": tc.function.name, \"arguments\": tc.function.arguments}\n",
    "                } for tc in tool_calls\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # Execute each requested tool and append the tool results\n",
    "        for tc in tool_calls:\n",
    "            name = tc.function.name\n",
    "            raw_args = tc.function.arguments or \"{}\"\n",
    "            try:\n",
    "                args = json.loads(raw_args)\n",
    "            except Exception as e:\n",
    "                tool_result = {\"error\": f\"invalid_json_arguments: {e.__class__.__name__}: {e}\", \"raw\": raw_args}\n",
    "            else:\n",
    "                try:\n",
    "                    fn = tool_registry[name]\n",
    "                    out = fn(args)\n",
    "                    tool_result = {\"ok\": True, \"result\": out}\n",
    "                except Exception as e:\n",
    "                    tool_result = {\"ok\": False, \"error\": f\"{e.__class__.__name__}: {e}\"}\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tc.id,\n",
    "                \"content\": json.dumps(tool_result)[:truncate_tool_result_chars],\n",
    "            })\n",
    "        \n",
    "        print(messages)\n",
    "\n",
    "    # If we exhaust steps, return partial\n",
    "    return {\"messages\": messages, \"final_text\": None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0caadc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageFunctionToolCall(id='call_MPXbOtqemK5BiksVZSYsCZH5', function=Function(arguments='{\"location\": \"Paris\", \"units\": \"celsius\"}', name='get_weather'), type='function'), ChatCompletionMessageFunctionToolCall(id='call_qfoEMFXTCfzPWPykwwa21BCX', function=Function(arguments='{\"place\": \"Louvre\", \"limit\": 2}', name='get_review'), type='function')]\n",
      "[{'role': 'system', 'content': 'You are a helpful travel assistant. Use tools when needed.'}, {'role': 'user', 'content': \"I'm going to Paris. What's the weather in celsius and give me 2 reviews of the Louvre?\"}, {'role': 'assistant', 'tool_calls': [{'id': 'call_MPXbOtqemK5BiksVZSYsCZH5', 'type': 'function', 'function': {'name': 'get_weather', 'arguments': '{\"location\": \"Paris\", \"units\": \"celsius\"}'}}, {'id': 'call_qfoEMFXTCfzPWPykwwa21BCX', 'type': 'function', 'function': {'name': 'get_review', 'arguments': '{\"place\": \"Louvre\", \"limit\": 2}'}}]}, {'role': 'tool', 'tool_call_id': 'call_MPXbOtqemK5BiksVZSYsCZH5', 'content': '{\"ok\": true, \"result\": 22}'}]\n",
      "[{'role': 'system', 'content': 'You are a helpful travel assistant. Use tools when needed.'}, {'role': 'user', 'content': \"I'm going to Paris. What's the weather in celsius and give me 2 reviews of the Louvre?\"}, {'role': 'assistant', 'tool_calls': [{'id': 'call_MPXbOtqemK5BiksVZSYsCZH5', 'type': 'function', 'function': {'name': 'get_weather', 'arguments': '{\"location\": \"Paris\", \"units\": \"celsius\"}'}}, {'id': 'call_qfoEMFXTCfzPWPykwwa21BCX', 'type': 'function', 'function': {'name': 'get_review', 'arguments': '{\"place\": \"Louvre\", \"limit\": 2}'}}]}, {'role': 'tool', 'tool_call_id': 'call_MPXbOtqemK5BiksVZSYsCZH5', 'content': '{\"ok\": true, \"result\": 22}'}, {'role': 'tool', 'tool_call_id': 'call_qfoEMFXTCfzPWPykwwa21BCX', 'content': '{\"ok\": true, \"result\": {\"place\": \"Louvre\", \"reviews\": [\"Review 1 for Louvre\", \"Review 2 for Louvre\"]}}'}]\n",
      "None\n",
      "FINAL:\n",
      " The current temperature in Paris is 22°C.\n",
      "\n",
      "Here are two reviews of the Louvre:\n",
      "\n",
      "1. **Review 1 for Louvre**: \"The Louvre is an absolute must-visit! The art collection is breathtaking, and the architecture of the museum itself is stunning. Make sure to allocate enough time to explore.\"\n",
      "\n",
      "2. **Review 2 for Louvre**: \"Visiting the Louvre was a dream come true. The Mona Lisa is smaller than I expected, but the experience of seeing it in person is unforgettable. Be prepared for crowds, though!\"\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful travel assistant. Use tools when needed.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm going to Paris. What's the weather in celsius and give me 2 reviews of the Louvre?\"}\n",
    "]\n",
    "\n",
    "result = run_agent(messages, TOOLS_SCHEMA, TOOLS_REGISTRY)\n",
    "print(\"FINAL:\\n\", result[\"final_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdc1a6",
   "metadata": {},
   "source": [
    "#### Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0b2cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _date(s: str) -> datetime:\n",
    "    \"\"\"Parse YYYY-MM-DD into a datetime.date object (as datetime).\"\"\"\n",
    "    return datetime.strptime(s, \"%Y-%m-%d\")\n",
    "\n",
    "def _geocode_open_meteo(city: str) -> Optional[Dict[str, float]]:\n",
    "    \"\"\"Geocode a city name to lat/lon using Open-Meteo's free geocoder.\"\"\"\n",
    "    r = requests.get(\n",
    "        \"https://geocoding-api.open-meteo.com/v1/search\",\n",
    "        params={\"name\": city, \"count\": 1},\n",
    "        timeout=20\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    res = r.json().get(\"results\") or []\n",
    "    if not res:\n",
    "        return None\n",
    "    x = res[0]\n",
    "    return {\"name\": x[\"name\"], \"lat\": x[\"latitude\"], \"lon\": x[\"longitude\"]}\n",
    "\n",
    "def get_daily_forecast(city: str, start: str, end: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    7-day window forecast for [start, end] using Open-Meteo forecast endpoint.\n",
    "    Returns daily tmax/tmin and precipitation probability.\n",
    "    \"\"\"\n",
    "    loc = _geocode_open_meteo(city)\n",
    "    if not loc:\n",
    "        return {\"mode\": \"forecast\", \"city\": city, \"error\": \"geocoding_failed\"}\n",
    "    r = requests.get(\n",
    "        \"https://api.open-meteo.com/v1/forecast\",\n",
    "        params={\n",
    "            \"latitude\": loc[\"lat\"],\n",
    "            \"longitude\": loc[\"lon\"],\n",
    "            \"daily\": \"temperature_2m_max,temperature_2m_min,precipitation_probability_mean,weathercode\",\n",
    "            \"start_date\": start,\n",
    "            \"end_date\": end,\n",
    "            \"timezone\": \"auto\",\n",
    "        },\n",
    "        timeout=25\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    d = r.json().get(\"daily\", {})\n",
    "    out = []\n",
    "    for i, day in enumerate(d.get(\"time\", [])):\n",
    "        out.append({\n",
    "            \"date\": day,\n",
    "            \"tmax\": d.get(\"temperature_2m_max\", [None])[i],\n",
    "            \"tmin\": d.get(\"temperature_2m_min\", [None])[i],\n",
    "            \"pop\": d.get(\"precipitation_probability_mean\", [None])[i],  # %\n",
    "            \"wcode\": d.get(\"weathercode\", [None])[i],\n",
    "        })\n",
    "    return {\"mode\": \"forecast\", \"city\": city, \"lat\": loc[\"lat\"], \"lon\": loc[\"lon\"], \"days\": out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2b57497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'forecast',\n",
       " 'city': 'New York City',\n",
       " 'lat': 40.71427,\n",
       " 'lon': -74.00597,\n",
       " 'days': [{'date': '2025-09-13',\n",
       "   'tmax': 26.0,\n",
       "   'tmin': 14.6,\n",
       "   'pop': 1,\n",
       "   'wcode': 3},\n",
       "  {'date': '2025-09-14', 'tmax': 27.1, 'tmin': 16.9, 'pop': 4, 'wcode': 3},\n",
       "  {'date': '2025-09-15', 'tmax': 26.8, 'tmin': 19.6, 'pop': 2, 'wcode': 3},\n",
       "  {'date': '2025-09-16', 'tmax': 23.4, 'tmin': 19.3, 'pop': 2, 'wcode': 3},\n",
       "  {'date': '2025-09-17', 'tmax': 22.8, 'tmin': 19.3, 'pop': 6, 'wcode': 2},\n",
       "  {'date': '2025-09-18', 'tmax': 23.3, 'tmin': 19.5, 'pop': 8, 'wcode': 2},\n",
       "  {'date': '2025-09-19', 'tmax': 24.5, 'tmin': 18.8, 'pop': 7, 'wcode': 1}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_daily_forecast('New York City', '2025-09-13', '2025-09-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f05035ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wmo_to_condition(\n",
    "    wcode: int | None,\n",
    "    precip_mm: float | None,\n",
    "    snow_mm: float | None,\n",
    "    cloudcover_mean: float | None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Map WMO weather code + precip/snow/cloudcover to a coarse label:\n",
    "    'sunny' | 'cloudy' | 'rainy' | 'snowy'.\n",
    "    \"\"\"\n",
    "    precip = (precip_mm or 0.0)\n",
    "    snow = (snow_mm or 0.0)\n",
    "\n",
    "    # Hard overrides by actual measured precip/snow\n",
    "    if snow >= 1.0:\n",
    "        return \"snowy\"\n",
    "    if precip >= 1.0:\n",
    "        return \"rainy\"\n",
    "\n",
    "    # WMO (fallback if precip is light/zero)\n",
    "    # 0: clear, 1-3: partly to overcast\n",
    "    # 45/48: fog, 51-67 drizzle/rain variants, 71-77 snow variants,\n",
    "    # 80-82 rain showers, 85-86 snow showers, 95-99 thunderstorms\n",
    "    if wcode is not None:\n",
    "        if wcode == 0:\n",
    "            return \"sunny\"\n",
    "        if 1 <= wcode <= 3 or wcode in (45, 48):\n",
    "            # Could refine with cloudcover if available\n",
    "            if cloudcover_mean is not None and cloudcover_mean <= 25:\n",
    "                return \"sunny\"\n",
    "            return \"cloudy\"\n",
    "        if (51 <= wcode <= 67) or (80 <= wcode <= 82) or (95 <= wcode <= 99):\n",
    "            return \"rainy\"\n",
    "        if (71 <= wcode <= 77) or (85 <= wcode <= 86):\n",
    "            return \"snowy\"\n",
    "\n",
    "    # Last resort: use cloudcover if we have it\n",
    "    if cloudcover_mean is not None:\n",
    "        return \"sunny\" if cloudcover_mean <= 25 else \"cloudy\"\n",
    "\n",
    "    return \"cloudy\"\n",
    "\n",
    "\n",
    "def _archive_range(lat: float, lon: float, start: str, end: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch daily weather archive for [start, end] at (lat, lon).\n",
    "\n",
    "    Returns a dict:\n",
    "      {\n",
    "        \"latitude\": ...,\n",
    "        \"longitude\": ...,\n",
    "        \"daily\": [\n",
    "          {\n",
    "            \"date\": \"YYYY-MM-DD\",\n",
    "            \"tmax\": float | None,\n",
    "            \"tmin\": float | None,\n",
    "            \"precip_mm\": float | None,\n",
    "            \"snow_mm\": float | None,\n",
    "            \"cloudcover_mean\": float | None,\n",
    "            \"weathercode\": int | None,\n",
    "            \"condition\": \"sunny\"|\"cloudy\"|\"rainy\"|\"snowy\"\n",
    "          }, ...\n",
    "        ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    # Ask for multiple daily variables. The archive API supports these.\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start,\n",
    "        \"end_date\": end,\n",
    "        \"daily\": \",\".join([\n",
    "            \"temperature_2m_max\",\n",
    "            \"temperature_2m_min\",\n",
    "            \"precipitation_sum\",\n",
    "            \"snowfall_sum\",\n",
    "            \"weathercode\",\n",
    "            \"cloudcover_mean\",\n",
    "        ]),\n",
    "        \"timezone\": \"auto\",\n",
    "    }\n",
    "    r = requests.get(\"https://archive-api.open-meteo.com/v1/archive\", params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "\n",
    "    d = j.get(\"daily\", {})\n",
    "    times = d.get(\"time\", []) or []\n",
    "    tmaxs = d.get(\"temperature_2m_max\", []) or []\n",
    "    tmins = d.get(\"temperature_2m_min\", []) or []\n",
    "    psums = d.get(\"precipitation_sum\", []) or []\n",
    "    ssums = d.get(\"snowfall_sum\", []) or []\n",
    "    wcodes = d.get(\"weathercode\", []) or []\n",
    "    clouds = d.get(\"cloudcover_mean\", []) or []\n",
    "\n",
    "    out = []\n",
    "    n = len(times)\n",
    "    for i in range(n):\n",
    "        date = times[i]\n",
    "        tmax = tmaxs[i] if i < len(tmaxs) else None\n",
    "        tmin = tmins[i] if i < len(tmins) else None\n",
    "        pmm  = psums[i] if i < len(psums) else None\n",
    "        smm  = ssums[i] if i < len(ssums) else None\n",
    "        w    = wcodes[i] if i < len(wcodes) else None\n",
    "        cc   = clouds[i] if i < len(clouds) else None\n",
    "\n",
    "        cond = _wmo_to_condition(\n",
    "            int(w) if w is not None else None,\n",
    "            float(pmm) if pmm is not None else None,\n",
    "            float(smm) if smm is not None else None,\n",
    "            float(cc)  if cc  is not None else None,\n",
    "        )\n",
    "\n",
    "        out.append({\n",
    "            \"date\": date,\n",
    "            \"tmax\": tmax,\n",
    "            \"tmin\": tmin,\n",
    "            \"precip_mm\": pmm,\n",
    "            \"snow_mm\": smm,\n",
    "            \"cloudcover_mean\": cc,\n",
    "            \"weathercode\": w,\n",
    "            \"condition\": cond\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"latitude\": j.get(\"latitude\"),\n",
    "        \"longitude\": j.get(\"longitude\"),\n",
    "        \"daily\": out\n",
    "    }\n",
    "\n",
    "def get_historical_weather_stats(city: str, start: str, end: str, years_back: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    For trips > 7 days away: sample the same date range over the last N years.\n",
    "    Returns wet/fair ratios + condition counts + average tmin/tmax across samples.\n",
    "    \"\"\"\n",
    "    loc = _geocode_open_meteo(city)\n",
    "    if not loc:\n",
    "        return {\"mode\": \"historical\", \"city\": city, \"error\": \"geocoding_failed\"}\n",
    "\n",
    "    s_dt = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    e_dt = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    span_days = (e_dt - s_dt).days + 1\n",
    "    this_year = datetime.now().year\n",
    "\n",
    "    # Aggregates\n",
    "    wet_days = fair_days = 0\n",
    "    cond_counts = {\"sunny\": 0, \"cloudy\": 0, \"rainy\": 0, \"snowy\": 0}\n",
    "    tmax_acc = 0.0\n",
    "    tmin_acc = 0.0\n",
    "    temp_samples = 0\n",
    "    total_samples = 0\n",
    "    per_year = []\n",
    "\n",
    "    for y in range(1, years_back + 1):\n",
    "        year = this_year - y\n",
    "        s_y = s_dt.replace(year=year)\n",
    "        e_y = s_y + timedelta(days=span_days - 1)\n",
    "        block = _archive_range(loc[\"lat\"], loc[\"lon\"], s_y.strftime(\"%Y-%m-%d\"), e_y.strftime(\"%Y-%m-%d\"))\n",
    "        days = block.get(\"daily\", [])\n",
    "\n",
    "        year_wet = year_fair = 0\n",
    "        year_cond = {\"sunny\": 0, \"cloudy\": 0, \"rainy\": 0, \"snowy\": 0}\n",
    "\n",
    "        for d in days:\n",
    "            precip = float(d.get(\"precip_mm\") or 0.0) + float(d.get(\"snow_mm\") or 0.0)\n",
    "            if precip >= 1.0:\n",
    "                wet_days += 1; year_wet += 1\n",
    "            else:\n",
    "                fair_days += 1; year_fair += 1\n",
    "            total_samples += 1\n",
    "\n",
    "            cond = d.get(\"condition\", \"cloudy\")\n",
    "            if cond in year_cond:\n",
    "                year_cond[cond] += 1\n",
    "                cond_counts[cond] += 1\n",
    "\n",
    "            # temps\n",
    "            if d.get(\"tmax\") is not None:\n",
    "                tmax_acc += float(d[\"tmax\"])\n",
    "                temp_samples += 1\n",
    "            if d.get(\"tmin\") is not None:\n",
    "                tmin_acc += float(d[\"tmin\"])\n",
    "\n",
    "        per_year.append({\n",
    "            \"year\": year, \"wet\": year_wet, \"fair\": year_fair, \"total\": year_wet + year_fair,\n",
    "            \"conditions\": year_cond\n",
    "        })\n",
    "\n",
    "    wet_ratio = (wet_days / total_samples) if total_samples else 0.0\n",
    "    fair_ratio = (fair_days / total_samples) if total_samples else 0.0\n",
    "    avg_tmax = (tmax_acc / temp_samples) if temp_samples else None\n",
    "    avg_tmin = (tmin_acc / temp_samples) if temp_samples else None\n",
    "\n",
    "    return {\n",
    "        \"mode\": \"historical\",\n",
    "        \"city\": city,\n",
    "        \"lat\": loc[\"lat\"], \"lon\": loc[\"lon\"],\n",
    "        \"years_back\": years_back,\n",
    "        \"samples\": total_samples,\n",
    "        \"summary\": {\n",
    "            \"wet_days\": wet_days,\n",
    "            \"fair_days\": fair_days,\n",
    "            \"wet_ratio\": wet_ratio,\n",
    "            \"fair_ratio\": fair_ratio,\n",
    "            \"avg_tmax\": avg_tmax,\n",
    "            \"avg_tmin\": avg_tmin,\n",
    "            \"conditions\": cond_counts\n",
    "        },\n",
    "        \"per_year\": per_year\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2eceef30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'historical',\n",
       " 'city': 'New York City',\n",
       " 'lat': 40.71427,\n",
       " 'lon': -74.00597,\n",
       " 'years_back': 5,\n",
       " 'samples': 35,\n",
       " 'summary': {'wet_days': 8,\n",
       "  'fair_days': 27,\n",
       "  'wet_ratio': 0.22857142857142856,\n",
       "  'fair_ratio': 0.7714285714285715,\n",
       "  'avg_tmax': 24.72,\n",
       "  'avg_tmin': 15.977142857142859,\n",
       "  'conditions': {'sunny': 11, 'cloudy': 12, 'rainy': 12, 'snowy': 0}},\n",
       " 'per_year': [{'year': 2024,\n",
       "   'wet': 0,\n",
       "   'fair': 7,\n",
       "   'total': 7,\n",
       "   'conditions': {'sunny': 3, 'cloudy': 4, 'rainy': 0, 'snowy': 0}},\n",
       "  {'year': 2023,\n",
       "   'wet': 2,\n",
       "   'fair': 5,\n",
       "   'total': 7,\n",
       "   'conditions': {'sunny': 2, 'cloudy': 2, 'rainy': 3, 'snowy': 0}},\n",
       "  {'year': 2022,\n",
       "   'wet': 2,\n",
       "   'fair': 5,\n",
       "   'total': 7,\n",
       "   'conditions': {'sunny': 2, 'cloudy': 3, 'rainy': 2, 'snowy': 0}},\n",
       "  {'year': 2021,\n",
       "   'wet': 3,\n",
       "   'fair': 4,\n",
       "   'total': 7,\n",
       "   'conditions': {'sunny': 1, 'cloudy': 0, 'rainy': 6, 'snowy': 0}},\n",
       "  {'year': 2020,\n",
       "   'wet': 1,\n",
       "   'fair': 6,\n",
       "   'total': 7,\n",
       "   'conditions': {'sunny': 3, 'cloudy': 3, 'rainy': 1, 'snowy': 0}}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_historical_weather_stats('New York City', '2025-09-13', '2025-09-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2580c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
